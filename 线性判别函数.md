

# 线性判别函数
![2018-11-09.00.23.12-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-09.00.23.12-image.png)

## 3.1 判别函数
### 3.1.1 判别函数的定义
直接用来对模式进行分类的准则函数。

若分属于$$ \omega_{1}, \omega_{2} $$的两类模式可用一方程$$ d(X)=0 $$来划分，那么称$$d(X)$$为判别函数，或称判决函数、决策函数。

例如，一个二维的两类判别问题，模式分布如图所示，这些分属于$$ \omega_{1}, \omega_{2} $$两类的模式可用一直线方程来划分:
$$ d(X)= \omega_{1}x_{1}+\omega_{2}x_{2}+\omega_{3} = 0 $$

![2018-11-09.10.30.16-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-09.10.30.16-image.png)

![2018-11-09.10.30.45-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-09.10.30.45-image.png)

## 3.2 线性判别函数
### 3.2.1 线性判别函数的一般形式
![2018-11-09.11.05.49-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-09.11.05.49-image.png)

### 3.2.1 线性判别函数的性质
1. 两类情况
![2018-11-09.11.27.17-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-09.11.27.17-image.png)

2. 多类情况

对于$$M$$个线性可分模式类，$$\omega_{1},\omega_{2},\dots ,\omega_{M}$$，有三种划分方式：
 - $$\omega_{i}/\overline{\omega_{i}}$$两分法：
 
 
 用线性判别函数将属于$$\omega_{i}$$类的模式与其余不属于$$\omega_{i}$$类的模式分开。
 
 ![2018-11-09.11.36.03-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-09.11.36.03-image.png)
 
 ![2018-11-09.12.19.10-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-09.12.19.10-image.png)
 - $$\omega_{i}/\omega_{j}$$两分法：
 
 
 一个判别界面只能分开两个类别，不能把其余所有的类别都分开。判决函数为：$$d_{ij}(X)=W_{ij}^{T}X$$。这里$$ d_{ji}=-d_{ij} $$。
 
 ![2018-11-09.12.34.17-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-09.12.34.17-image.png)
 
 ![2018-11-09.12.36.28-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-09.12.36.28-image.png)
 
 分类时：每分离出一类，需要与$$I$$有关的$$M-1$$个判别函数；要分开$$M$$类模式，供需$$M(M-1)/2$$个判别函数。对三类问题需要3(3-1)/2=3个判别函数。即：每次从$$M$$类中去除两类的组合。
 
 ![2018-11-13.09.32.46-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.09.32.46-image.png)
 
 - $$\omega_{i}/\omega_{j}$$两分法特例
 
 
 当$$\omega_{i}/\omega_{j}$$两分法中的判别函数，可以分解为：$$d_{ij}(X)=d_{i}(X)-d_{j}(X)$$时，那么$$d_{i}(X)>d_{j}(X)$$就相当于多类情况2中的$$d_{ij}(X)>0$$
 
 ![2018-11-13.09.48.47-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.09.48.47-image.png)
 
 ![2018-11-13.09.49.28-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.09.49.28-image.png)
 
 ![2018-11-13.09.50.02-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.09.50.02-image.png)
 
 3. 小结
 	- 明确概念：线性可分
    
    一旦线性判别函数的系数$$W_{k}$$被确定以后，这些函数就可以作为模式分类的基础。
    -  $$\omega_{i}/\overline{\omega_{i}}$$与$$\omega_{i}/\omega_{j}$$分法的比较
    ![2018-11-13.09.59.17-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.09.59.17-image.png)
    
## 3.3 广义线性判别函数

### 3.3.1 广义线性判别函数的局限
- 以下情况不能使用线性判别函数<br>
![2018-11-13.10.04.45-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.10.04.45-image.png)

要达到分类效果，需要设计这样的判别函数：$$d(x)=(x-a)(x-b)$$

决策规则为![2018-11-13.10.06.31-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.10.06.31-image.png)

### 3.3.2 目的
对非线性边界：通过某映射，把模式空间$$X$$变成$$X^{*}$$，以便将$$X$$空间中线性可分的模式集，变成在$$X^{*}$$空间中线性可分的模式集。

### 3.3.3 非线性判别函数
非线性判别函数的形式之一就是非线性多项式函数。
设一训练用模式集，$$\left \{ X \right \}$$在模式空间X中线性不可分，非线性判别函数形式如下：

$$d(X)=w_{1}f_{1}(X)+w_{2}f_{2}(X)+\dots+w_{k}f_{k}(X)+w_{k+1}=\sum_{i=1}^{k+1}w_{i}f_{i}(X)$$

式中$$\left \{ f_{i}(x), I=1,2,\dots,k \right \}$$时模式$$X$$的单值实函数，$$f_{k+1}(X)=1$$。$$f_{i}(X)$$取什么形式及$$d(X)$$取多少项，取决于非线性边界的复杂程度。

### 3.3.4 广义线性判别函数
设n维训练用模式集{X}在模式空间X中线性不可分，其非线性判别函数形式为：

$$d(X)=w_{1}f_{1}(X)+w_{2}f_{2}(X)+\dots+w_{k}f_{k}(X)+w_{k+1}=\sum_{i=1}^{k+1}w_{i}f_{i}(X)$$

为了使上式变成线性函数，必须要将模式向量X变换到新的空间。为此，定义一个新的模式向量$$X^{*}$$，它的分量等于$$f_{i}(X)$$，即

$$X^{*}=\left [ x^{*}_{1},x^{*}_{2},\dots,x^{*}_{k},1 \right ]=\left [ f_{1}(X),f_{2}(X),\dots,f_{k}(X),1 \right ]$$

这里$$X^{*}$$空间的维数k高于X空间的维数n，上式可以写为：

$$d(X)=W^{T}X^{*}=d(X^{*}), W=\left [ w_{1},w_{2},\dots,w_{k},w_{k+1} \right ]^{T}$$

上式是线性的，完成了从非线性判别函数到线性判别函数的转化。

带来的问题：
- 非线性变换可能非常复杂
- 维数大大增加：维数灾难

![2018-11-13.11.05.09-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.11.05.09-image.png)

## 3.4 线性判别函数的几何性质
### 3.4.1 模式空间与超平面
- 模式空间：以n维模式向量X的n个分量为坐标变量的欧式空间。
- 模式空间的表示：点、有向线段
- 线性分类：用$$d(X)$$进行分类，相当于用超平面$$d(X)=0$$把模式空间分成不同的决策区域

设判别函数：$$ d(X)=W^{T}_{0}X+w_{n+1} $$

式中，$$W_{0}=[W^{T}_{1},W^{T}_{2},\dots, W^{T}_{n}]^{T}$$，$$X=[x_{1},x_{2},\dots,x_{n}]^{T}$$.

超平面：$$ d(X)=W^{T}_{0}X+w_{n+1} $$



![2018-11-13.14.18.09-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.14.18.09-image.png)

![2018-11-13.14.22.16-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.14.22.16-image.png)

![2018-11-13.14.24.14-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.14.24.14-image.png)

### 3.4.2 权空间和权向量解
设有线性判别函数：$$d(X)=w_{1}x_{1}+w_{2}x_{2}+\dots+w_{n}x_{n}+w_{n+1}$$，则以$$w_{1},w_{2},\dots,w_{n}.w_{n+1}$$为坐标变量的（n+1）维欧式空间称为权空间。

在权空间里，增广权空间$$W=[w_{1},w_{2},\dots,w_{n},w_{n+1}]$$对应该控件的一个点，也可以表示称为从元代你到这个点的一条有向线段。

当线性可分时，判别函数形式已定，只需要确定一个符合条件的权向量，也就是确定权向量的所有分量的具体值。

设$$X_{11}，X_{12},\dots，X_{1p}$$是属于$$w_{1}$$类的$$p$$个增广样本向量,$$X_{21},X_{22},\dots,X_{2q}$$属于$$w_{2}$$类的$$q$$个增广样本向量，并设两类问题的线性判别函数为$$d(X)=W^{T}X$$。根据$$d(X)$$的性质，要使$$d(X)$$把$$w_{1}$$类和$$w_{2}$$类分开，必须使下面$$(p+q)$$个不等式成立:

$$d(X_{1i})> 0,i=1,2,\dots,p$$

$$d(-X_{2i})> 0,i=1,2,\dots,q$$

样本的规范化，X称为规范化增广样本向量：
$$d(X)>0$$，其中$$X=\left\{\begin{matrix}
X_{1i}, i=1,2,\dots,p \\ 
-X_{2i}，i=1,2,\dots,q
\end{matrix}\right.$$

### 3.4.3 二分法
所谓二分法是指用判别函数$$d(X)$$把给定的N个模式分成两类的方法，二分法是一种基本的分类方法。判别函数将一定的模式集分割成两类的各种可能方法的总数反映了判别函数的二分能力，不同形式的判别函数对给定的模式集的不同分类能力可以通过二分法总数的多少来衡量。

## 3.5 感知器算法
对于线性判别函数，当模式维数已知，判别函数的形式实际上已经确定，如：三维时

$$d(X)=w_{1}x_{1}+w_{2}x_{2}+w_{3}x{3}+w_{4}=W_{T}X$$，其中$$X=[x_{1},x_{2},x_{3},1]^{T},W=[w_{1},w_{2},w_{3},w_{4}]^{T}$$

只要求出权向量，分类器的设计即告完成。

### 3.5.1 概念理解
1. 训练与学习

- 训练：用已知类别的模式样本知道机器对分类规则进行反复修改，最终使分类结果与已知类别信息完全相同的过程

- 学习：从分类器的角度讲：监督学习<-->训练

2. 确定性分类器

处理确定可分情况的分类器。通过几何方法将特征空间分解为对应不同类的子空间，又称为几何分类器。

3. 感知器

对一种分类学习机模型的称呼，属于有关机器学习的仿生学领域中的问题，由于无法实现非线性分类而下马。但“赏罚概念”得到广泛应用。

### 3.5.2 感知器学习
![2018-11-13.15.58.46-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.15.58.46-image.png)

![2018-11-13.15.59.44-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.15.59.44-image.png)

### 3.5.3 感知器算法步骤
![2018-11-13.16.01.14-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.16.01.14-image.png)
![2018-11-13.16.01.40-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.16.01.40-image.png)

感知器算法是一种赏罚过程：
- 分类正确时，对权向量“赏”——权向量不变
- 分类错误时，对权向量“罚”——对其修改，向正确的方向转换

### 3.5.4 收敛性
收敛性：经过算法的有限次迭代运算后，求出一个使所有样本都能正确分类的$$W$$，则称算法是收敛的。

收敛条件：模式类别线性可分。

### 3.5.5 举例
![2018-11-13.16.10.21-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.16.10.21-image.png)

![2018-11-13.16.10.43-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.16.10.43-image.png)

![2018-11-13.16.11.37-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.16.11.37-image.png)

![2018-11-13.16.11.59-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.16.11.59-image.png)

### 3.5.6 感知器算法用于多类情况
![2018-11-13.22.33.55-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.22.33.55-image.png)

![2018-11-13.22.38.47-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.22.38.47-image.png)

#### 举例
![2018-11-13.22.40.00-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.22.40.00-image.png)

![2018-11-13.22.40.35-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.22.40.35-image.png)

![2018-11-13.22.41.38-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.22.41.38-image.png)

![2018-11-13.22.41.59-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.22.41.59-image.png)

## 3.6 梯度法
### 3.6.1 梯度法基本原理
#### 1. 梯度的概念
函数在某点的梯度使这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。记为：

![2018-11-13.23.26.44-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.23.26.44-image.png)

即：<br>
梯度的方向是函数$$f(Y)$$在$$Y$$点增长最快的方向，<br>
梯度的模是$$f(Y)$$在增长最快的方向上的增长率（增长率最大值）。

显然：<b>负梯度指出最陡下降方向。</b>————梯度算法的依据。
  
![2018-11-13.23.31.27-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.23.31.27-image.png)
  
#### 2. 梯度算法
设两个线性可分的模式类$$w_{1}$$和$$w_{2}$$的样本共$$N$$个，$$w_{2}$$类样本乘$$(-1)$$。将两类样本分开的判决函数$$d(X)$$应满足：
$$d(X_{i})=W^{T}X_{i}>0,i=1,2,\dots,N$$,共$$N$$个不等式。

梯度算法的目的仍然是求一个满足上述条件的权向量，主导思想是将联立不等式求解$$W$$的问题，转换成求准则函数极小值的问题。

用负梯度向量的值对权向量$$W$$进行修正，实现使准则函数达到极小值的目的。

准则函数的选取原则： 具有唯一的最小值，并且这个最小值发生在$$W^{T}X_{i}>0$$时。

##### 基本思路
定义一个对错误分类敏感的准则函数$$J(W,X)$$，在$$J$$的梯度 方向上对权向量进行修改。一般关系表示成从$$W(k)$$导出$$W(k+1)$$：

![2018-11-13.23.44.25-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.23.44.25-image.png)

其中c是正的比例因子。
##### 梯度法求解步骤
![2018-11-13.23.45.48-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.23.45.48-image.png)
![2018-11-13.23.46.07-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.23.46.07-image.png)
![2018-11-13.23.46.29-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.23.46.29-image.png)

##### 说明
![2018-11-13.23.50.03-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-13.23.50.03-image.png)

### 3.6.2 固定增量法
准则函数：$$J(W,X)=\frac{1}{2}(\left | w^{T}X \right |-W^{T}X)$$

该准则函数有唯一最小值"0"，且发生在$$W^{T}X>0$$的时候。

#### 求W(k)的递推公式
$$W_{0}=[w_{1},w_{2},\dots,w_{n},w_{n+1}]^{T}$$，$$X=[x_{1},x_{2},\dots,x_{n},1]^{T}$$.

![2018-11-14.00.09.48-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.00.09.48-image.png)

![2018-11-14.00.29.13-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.00.29.13-image.png)
![2018-11-14.00.10.10-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.00.10.10-image.png)

![2018-11-14.00.28.33-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.00.28.33-image.png)

![2018-11-14.00.28.49-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.00.28.49-image.png)

## 3.7 最小平方误差算法
上述的感知器算法、梯度算法、固定增量算法或其他类似方法，只有当模式类可分离时才收敛，在不可分的情况下，算法会来回摆动，始终不收敛。当一次次迭代而又不见收敛时，造成不收敛现象的原因分不清，有两种可能：
- 迭代过程本身收敛缓慢
- 模式本身不可分

### 3.7.1 LMSE算法特点
1. 对可分模式收敛
2. 对于类别不可分的情况也能指出来

### 3.7.2 LMSE算法
1. 原理
![2018-11-14.09.59.19-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.09.59.19-image.png)

![2018-11-14.10.02.04-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.10.02.04-image.png)

2. 算法出发点
选择一个准则函数，使得当$$J$$达到最小值时，$$XW=B$$可得到近似解（最小二乘近似解）

准则函数定义为：$$J(W,X,B)=\frac{1}{2}\left \| XW-B \right \|^{2}$$

"最小二乘"：
- 最小：使方程组两边误差最小，也即使$$J$$最小
- 二乘：次数为2

3. 算法的思路<br>
![2018-11-14.10.43.18-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.10.43.18-image.png)

![2018-11-14.10.44.48-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.10.44.48-image.png)

可以看出：
- 当函数$$J$$达到最小值，等式$$XW=B$$有最优解。即又将问题转化为求准则函数极小值的问题。
- 因为$$J$$有两个变量$$W$$和$$B$$，有更多的自由度供选择求解，故可望改善算法的收敛速度。

4. 递推公式
![2018-11-14.10.53.44-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.10.53.44-image.png)

![2018-11-14.11.09.41-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.11.09.41-image.png)

![2018-11-14.10.54.07-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.10.54.07-image.png)

$$X^{\#}$$是$$X$$的伪逆，也称广义逆阵。$$X$$为$$N\times (n+1)$$长方阵，$$X^{\#}$$是$$(n+1)\times N$$长方阵.

![2018-11-14.11.09.29-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.11.09.29-image.png)

![2018-11-14.11.10.02-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.11.10.02-image.png)

![2018-11-14.11.22.38-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.11.22.38-image.png)

5. 模式类别可分性判别

可以证明：当模式类线性可分，且校正系数$$c$$满足$$0<c<1$$时，该算法收敛，可求得解$$W$$。

理论上不能证明该算法到底需要迭代多少步才能达到收敛，通常在每次迭代计算后检查一下$$XW(k)$$和误差向量$$e(k)$$， 从而可以判断是否收敛。

![2018-11-14.12.17.07-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.12.17.07-image.png)

综上所述：只有当$$e(k$$)中有大于零的分量时，才需要继续迭代，一旦$$e(k)$$的全部分量只有0和负数，则立即停止。事实上，往往早在$$e(k)$$全部分量都达到非正值以前，就能看出其中有些分量向正值变化得极慢，可及早采取对策。

通过反证法可以证明：在线性可分情况下，算法进行过程中不会出现$$e(k)$$的分量全为负的情况；若出现$$e(k)$$的分量全为负，则说明模式类线性不可分。

6.  LMSE算法描述

![2018-11-14.12.20.14-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.12.20.14-image.png)

![2018-11-14.12.20.25-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.12.20.25-image.png)

![2018-11-14.12.20.38-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.12.20.38-image.png)

![2018-11-14.12.20.48-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.12.20.48-image.png)

![2018-11-14.12.21.07-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.12.21.07-image.png)

7. 算法特点
- 算法尽管略为复杂一些，但提供了线性可分的测试特征。
- 同时利用N个训练样本，同时修改$$W$$和$$B$$，故收敛速度快。
- 计算矩阵$$(X^{T}X)^{-1}$$复杂，但可用迭代算法计算。

8. 举例1
![2018-11-14.12.23.38-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.12.23.38-image.png)

![2018-11-14.12.23.54-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.12.23.54-image.png)

![2018-11-14.12.24.53-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.12.24.53-image.png)

![2018-11-14.12.29.30-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.12.29.30-image.png)

![2018-11-14.12.30.17-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.12.30.17-image.png)

![2018-11-14.12.30.52-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.12.30.52-image.png)

9. 举例2

![2018-11-14.12.32.08-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.12.32.08-image.png)

![2018-11-14.12.32.18-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-14.12.32.18-image.png)

## 小结
1. 感知器法、梯度法、最小平方误差算法讨论的分类算法都是通过模式样本来确定判别函数的系数，所以要使一个分类器设计完善，必须采用有代表性的数据，训练判别函数的权系数。它们能合理反映模式数据的总体。
2. 要获得一个有较好判别性能的线性分类器，所需要的训练样本的数目的确定。 <br>
	用指标二分法能力$$N_{0}$$来确定训练样本的数目：$$N_{0}$$=2(n+1)$$
    
通常训练样本的数目不能低于$$N_{0}$$，选为$$N_{0}$$的5~10倍左右。 

- 二维：不能低于6个样本，最好选在30~60个样本之间。 
- 三维：不能低于8个样本，最好选在40~80个样本之间。