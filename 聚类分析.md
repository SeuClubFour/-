
# 二. 聚类分析
## 2.1 聚类分析的概念
### 2.1.1距离的理解
当研究一个复杂对象的时候，可以对其特征进行各种可能的测量，将各种测量值组成向量形式，成为该样本的特征向量。它相当于特征空间的一个点，整个模式样本集的特征向量可以看成分布在特征空间中的一些点。

特征空间中点与点之间的距离函数作为模式相似性的测量，以“距离”作为模式分类的依据，距离越小，越“相似”

### 2.1.2 合适的特征
聚类分析是按照不同对象之间的差异，根据距离函数的规则做模式分类的，这种方法是否有效与模式特征向量的分布形式有很大的关系。

合适的特征：
- 使得模式样本的分布呈现一定的规则
- 不好的特征：围棋子的大小

### 2.1.3聚类分析的两个问题
 1. 特征的选择：略去多余的特征
 2. 测量值的量化：
   - 连续值的量化
   - 量级的数量化
   - 定性的指标
   
## 2.2 相似性测度和聚类准则
### 2.2.1相似性测度
相似性测度是衡量模式之间相似性的一种尺度。距离就是一种相似性的测度，它可以用来度量同一类样本之间的类似性和不属于同一类的样本之间的差异性。

#### 1. 欧式距离
![2018-11-08.09.59.07-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.09.59.07-image.png)

> 1. 模式特征向量的构成：各特征向量对应的维上应当是相同的物理量，而且要注意物理量的单位。某些维上物理量采用的单位发生变化，会导致对同样的点集出现不同聚类结果的现象
> 2. 解决方法：使特征数据标准化，使其与变量的单位无关。此时所描述的点始终相对的位置关系，只要样本点间的相对位置关系不变。同样地，如果对数据不满意，感觉所有数值整体上都太大或太小，也可以采用类似的处理方法。

#### 2. 马氏距离
![2018-11-08.10.07.52-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.10.07.52-image.png)
![2018-11-08.10.08.18-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.10.08.18-image.png)

表示的概念是各分量上模式样本到均值的距离，也就是再各维上模式的分散情况。$$ \delta _{jk}^{2} $$越大，离均值越远。

欧式距离类似于两个点之间的直线距离，而马氏距离更类似于等高线；欧式距离相等代表集合距离上的相等，马氏距离的相等代表处于同一等高线，也就是同一分布水平上。

马氏距离的计算是建立在总体样本的基础上的。即，相同的两个样本，放入两个不同的总体中，最后计算得出的两个样本间的马氏距离通常是不同的。

当$$ C=I $$时，马氏距离为欧式距离。

模式向量$$X$$到均值向量$$M$$的马氏距离表示的是$$X$$与该模式类的相似性的大小，马氏距离越小，说明模式$$X$$与该模式类的相似程度越大，反之，说明相似程度越小。

马氏距离的优点是排除了模式样本之间的相关性影响。

#### 3. 明氏距离
![2018-11-08.10.19.16-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.10.19.16-image.png)

#### 4. 汉明距离
![2018-11-08.10.27.42-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.10.27.42-image.png)
汉明距离的值为两个模式向量的取值不同的分量的数目

#### 5. 角度相似性函数
![2018-11-08.10.29.47-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.10.29.47-image.png)

夹角余弦的测度反映了几何上相似性的特征，它对于坐标系的旋转及放大缩小是不变的，但对位移和一般的线性变换并不具有不变性的性质。

当特征的取值仅为0、1二值时，夹角余弦度量$$S(X_{i},X_{j})$$等于$$X_{i}、X_{j}$$中具有共有特征数目的相似性测度。

### 2.2.2 聚类准则
聚类准则：根据相似性测度确定的，衡量模式之间是否相似的标准。即把不同模式聚为一类还是归为不同类的准则。

聚类准则的确定有阈值准则和函数准则两种方式。
#### 1. 阈值准则
根据规定的距离阈值进行分类的准则。例如近邻聚类

#### 2. 函数准则
利用聚类准则函数进行分类的准则。

模式类别之间的相似性或差异性可用一个函数来表示。再聚类分析中，表示模式类间的相似性或差异性的函数称为聚类准则函数。

它应是模式样本集$$\left \{ X \right \}$$和模式类别$$\left \{ S_{j},j=1,2,......,c \right \}$$的函数。可使聚类分析转化为寻找准则函数极值的最优化问题。一种常用的指标使误差平方和。

一种常用的指标函数是误差平方和，
![2018-11-08.11.03.41-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.11.03.41-image.png)
![2018-11-08.11.08.09-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.11.08.09-image.png)
![2018-11-08.11.09.28-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.11.09.28-image.png)

## 2.3 基于距离阈值的聚类算法
### 2.3.1 近邻聚类法
![2018-11-08.13.53.37-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.13.53.37-image.png)

![2018-11-08.13.54.29-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.13.54.29-image.png)

![2018-11-08.13.55.27-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.13.55.27-image.png)

![2018-11-08.13.56.49-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.13.56.49-image.png)

### 2.3.2 最大最小距离法
![2018-11-08.13.57.54-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.13.57.54-image.png)
![2018-11-08.14.02.59-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.14.02.59-image.png)
![2018-11-08.14.02.21-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.14.02.21-image.png)

<b>思路总结：先找中心后分类；关键：怎么开新类，聚类中心如何定。为使聚类中心更具有代表性，可取各类样本均值作为聚类中心</b>

![2018-11-08.14.13.49-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.14.13.49-image.png)

## 2.4 层次聚类法
<b>思路:</b>每个样本自成一类，然后按照距离准则逐步合并，减少类数

1. 算法描述
![2018-11-08.14.20.21-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.14.20.21-image.png)
![2018-11-08.14.20.52-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.14.20.52-image.png)

><b>结束条件：</b>
>1. 取距离阈值$$T$$，当$$D(n)$$的最小分量超过给定值$$T$$时，算法停止。夺得即为聚类结果。
>2. 或不设阈值$$T$$，一直将全部样本聚为一类为止，输出聚类的分级树。

2. 问题讨论：类间距离计算准则
	1. 最短距离法
    ![2018-11-08.14.49.19-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.14.49.19-image.png)
    ![2018-11-08.14.50.28-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.14.50.28-image.png)
    2. 最长距离法
    ![2018-11-08.14.50.56-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.14.50.56-image.png)
    3. 重心法
    ![2018-11-08.14.52.13-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.14.52.13-image.png)
    4. 类平均距离法
    ![2018-11-08.14.52.51-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.14.52.51-image.png)
## 2.5 动态聚类法
![2018-11-08.14.59.36-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.14.59.36-image.png)

两种常用算法：
 - K-均值算法
 - 迭代自组织的数据分析算法（ISODATA，iterative self-organizing data analysis techniques algorithm）
 
### 2.5.1 K-均值算法
#### 条件及约定：
- 设待分类的模式特征矢量集为$$ \left \{ X_{1}, X_{2}, ...,X_{N} \right \}$$
- 类的数目K是事先取定的
    
#### 基本思想：
- 首先任意选取K个聚类中心，按最小距离原则将个模式分配到K类的某一类
- 不断计算聚类中心和调整个模式的类别，最终使个模式到其判属类别中心的距离平方之和最小
    
#### 基于使聚类准则函数最小化
- 准则函数：聚类集中每一样本点到该类中心的距离平方和
![2018-11-08.16.54.15-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.16.54.15-image.png)

- K-均值算法的聚类准则：聚类中心的选择应使准则函数$$J$$极小，即使$$J_{j}$$的值极小。

![2018-11-08.16.57.51-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.16.57.51-image.png)

#### 算法步骤
![2018-11-08.17.01.29-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.17.01.29-image.png)

#### 算法讨论
结果收到所选聚类中心的个数和其初始位置，以及模式样本的几何性质及读入次序等的影响。实际应用中需要试探不同的K值和选择不同的聚类中心起始值。

#### 算法实例
![2018-11-08.17.04.31-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.17.04.31-image.png)

![2018-11-08.17.04.50-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.17.04.50-image.png)

![2018-11-08.17.05.02-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.17.05.02-image.png)

![2018-11-08.17.05.15-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.17.05.15-image.png)

![2018-11-08.17.05.30-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.17.05.30-image.png)

#### 类别数目未知
- 在类别墅未知情况下使用K-均值算法，可以假设类别数是逐步增加的，显然准则函数是随K的增加而单调地减少的。
- 如果样本集的合理聚类数为K类，当类别数从1增加到K时，准则函数会迅速减小，当类别数超过K时，准则函数虽然继续减少但会呈现平稳趋势。
![2018-11-08.17.08.22-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.17.08.22-image.png)

#### 如何避免初始聚类中心的影响
- 多次运行K均值算法，例如20~1000次，每次随机选取不同的初始聚类中心
- 聚类结束后计算准则函数值
- 选取准则函数值最小的聚类结果为最后的结果
- 该方法一般适用于聚类数目小于10的情况

### 2.5.2 ISDATA算法
#### 基本思路
1. 选择初始控制参数。可选不同的指标，也可在迭代过程中人为修改，以将N个模式样本按指标发呢配到各个聚类中心中去
2. 计算个类中诸样本的距离指标函数
3. 获取新的聚类中心
4. 分裂处理
5. 合并处理
6. 重新进行迭代运算，计算各项指标，判断聚类结果是否符合要求，若不符合，返回2，经过多次迭代后，若结果收敛，则运算结束

#### ISODATA算法的步骤
![2018-11-08.17.17.36-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.17.17.36-image.png)

![2018-11-08.17.17.52-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.17.17.52-image.png)

![2018-11-08.17.19.13-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.17.19.13-image.png)

![2018-11-08.17.19.35-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.17.19.35-image.png)
$$ \overline{D_{j}}=\frac{1}{N_{j}}\sum_{X\in Z_{j}}^{ }\left \| X-Z_{j} \right \| $$

![2018-11-08.17.45.12-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.17.45.12-image.png)
$$ \overline{D}=\frac{1}{N}\sum_{j=1}^{N_{C}}\sum_{X\in S_{j}}^{ }\left \| X-Z_{j} \right \| = \frac{1}{N}\sum_{j=1}^{N_{C}}N_{j}\overline{D_{j}} $$

![2018-11-08.17.49.30-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.17.49.30-image.png)

![2018-11-08.17.49.49-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.17.49.49-image.png)

![2018-11-08.17.50.24-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.17.50.24-image.png)
$$ D_{ij}=\left \| Z_{i}-Z_{j} \right \|,i=1,2,\cdots ,N_{c-1};j=i+1,\cdots N_{c} $$

![2018-11-08.17.53.45-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.17.53.45-image.png)
$$ D_{i_{1}j_{1}}<D_{i_{2}j_{2}}<\cdots <D_{i_{L}j_{L}} $$
该队列最大个数时控制合并对数的参数L

![2018-11-08.17.58.33-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.17.58.33-image.png)
## 2.6 基于密度的聚类算法
只要一个区域中的点的密度大于某个阈值，就把他加到与之相近的聚类中去。对于簇中每个对象，在给定的半径ε的领域中至少要包含最小数数目个对象。

这类算法能够克服基于距离的算法只能发现“类圆形”的聚类的缺点，可以发现任意形状的聚类，且对噪声数据不敏感。

#### 2.6.1 DBSCAN算法
DBSCAN(Density-Based Spatial Clustering of Applications with Noise)一个比较有代表性的基于密度的聚类算法。与层次聚类方法不同，它将簇定义为密度相连的带你的最大集合，能够把具有足够高密度的区域划分成簇，并可在有“噪声”的空间数据库中发现任意形状的聚类。

![2018-11-08.20.05.30-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.20.05.30-image.png)

##### 密度的定义
- 基于中心的密度定义：数据集特定点的密度通过该点$$Eps$$半径之内的点基数（包括本身）来估计。显然，密度依赖于半径。
![2018-11-08.20.15.48-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.20.15.48-image.png)

- 基于密度定义，我们将点分为：
	- 稠密区域内部的点（核心点）：在半径$$Eps$$内含有超过$$MinPts$$数目的点，则该点为核心点
    - 稠密区域边缘上的点（边界点）：在半径$$Eps$$内点的数量小于$$MinPts$$，但是在核心点的邻居
    - 稀疏区域中的点（噪声或背景点）：任何不是核心点或边界点的点
    ![2018-11-08.20.23.35-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.20.23.35-image.png)
    ![2018-11-08.20.30.20-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.20.30.20-image.png)
    
- 直接密度可达：给定一个对象集合$$D$$，如果$$p$$在$$q$$的$$Eps$$领域内，而$$q$$是一个核心对象，则称对象$$p$$从对象q出发时是直接密度可达的
- 密度可达：如果存在一个对象链$$ p_{1},p_{2},\cdots,p_{n},p_{1}=q,p_{n}=p $$,对于$$ p_{i}\in D(1 \leq i  \leq n)$$，$$ p_{i+1} $$是从$$ p_{i} $$关于$$Eps$$和$$MinPts$$直接密度可达的，则对象$$p$$是从对象$$q$$关于$$Eps$$和$$MinPts$$密度可达的。$$q$$是核心对象。
- 密度相连：如果存在对象$$O \in D$$，使对象$$p$$和$$q$$都是从$$O$$关于$$Eps$$和$$MinPts$$密度可达的，那么对象$$p$$到$$q$$是关于$$Eps$$和$$MinPts$$密度相连的。

![2018-11-08.21.08.19-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.21.08.19-image.png)

##### DASCAN算法原理
- DBSCAN通过检查数据集中每个点的$$Eps$$邻域来搜索簇，如果点$$p$$的$$Eps$$邻域包含的点多于$$MinPts$$个，则创建一个以$$p$$为核心对象的簇。
- 然后，DBSCAN迭代地聚集从这些核心对象直接密度可达的对象，这个过程可能涉及一些密度可达簇的合并。
- 当没有新的点添加到任何簇时，该过程结束。

##### DBSCAN算法描述
![2018-11-08.21.24.46-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.21.24.46-image.png)

##### DBSCAN运行效果
![2018-11-08.21.25.33-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-08.21.25.33-image.png)

##### DBSCAN算法的优缺点
- 优点：基于密度的定义，相对抗噪音，能处理任意形状和大小的簇
- 缺点：
	- 当簇的密度变化太大时，会有麻烦
    - 对于高维问题，密度定义是个比较麻烦的问题
    - 参数的确定需要尝试或者先验知识
    
## 2.7 聚类结果的评价
1. 聚类中心之间的距离：距离越大，通常可考虑分为不同类
2. 聚类域中的样本数目：样本数目少且聚类中心距离远，可考虑是否为噪声
3. 聚类域内样本的距离方差：方差过大的样本可考虑是否属于这一类