# 特征提取
## 5.1 基本概念
### 5.1.1 特征提取的必要性
- 在很多实际问题中，往往不容易找到那些最重要的特征，或受客观条件的限制，不能对它们进行有效的测量；
- 在测量时，由于人们心理上的作用，只要条件许可总希望把特征取得多一些；
- 由于客观上的需要，为了突出某些有用信息，抑制无用信息，有意加上一些比值、指数或对数等组合计算特征；
- 如果将数目很多的测量值不做分析，全部直接用作分类特征，不但耗时，而且会影响到分类的效果，产生“特征维数灾难”问题。

为了设计出效果好的分类器，通常需要对原始的测量值集合进行分析，经过选择或变换处理，组成有效的识别特征；在保证一定分类精度的前提下，减少特征维数，即进行“降维”处理，使分类器实现快速、准确和高效的分类。

为达到上述目的，关键是所提供的识别特征应具有很好的可分性，使分类器容易判别。为此，需对特征进行选择。 
- 应去掉模棱两可、不易判别的特征
- 所提供的特征不要重复，即去掉那些相关性强且没有 增加更多分类信息的特征。

### 5.1.2 特征选择和特征提取的区别
1. 特征选择：从$$n$$个度量值集合$$\left \{ x_{1}, x_{2},…, x_{n} \right \}$$中，按某一准则选取出$$m$$维$$(m<n)$$供分类用的子集，作为降维的分类特征；

2. 特征提取：使$$\left \{ x_{1}, x_{2},…, x_{n} \right \}$$通过某种变换，产生$$m$$个特征$$( y_{1}, y_{2},…, y_{n} )(m<n)$$，作为新的分类特征（或称为二次特征）； 

3. 其目的都是为了在尽可能保留识别信息的前提下，降低特征空间的维数，已达到有效的分类。

- 特征选择：一般根据物理特征或结构特征进行压缩。
- 特征提取：一般用数学的方法进行压缩。

## 5.2 类别可分性判据
### 5.2.1 基于几何距离的可分性判据
一般来讲，不同类的模式可以被区分是由于它们所属类别在特征空间中的类域是不同的区域。 

显然，区域重叠的部分越小或完全没有重叠，类别的可分性就越好。 

因此可以用距离或离差测度（散度）来构造类别的可分性判据。

#### 5.2.1.1 点与点的距离
![2018-11-15.21.49.28-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-15.21.49.28-image.png)

#### 5.2.1.2 点与点集的距离
用均方欧式距离表示：<br>
![2018-11-15.21.58.55-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-15.21.58.55-image.png)

#### 5.2.1.3 类内及总体的均值矢量
类的均值矢量：![2018-11-15.22.00.26-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-15.22.00.26-image.png)

各类模式的总体均值矢量：![2018-11-15.22.00.59-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-15.22.00.59-image.png)

$$P_{i}$$为相应类的先验概率，当用统计量代替先验概率时，总体均值矢量可表示为： 
![2018-11-15.22.03.32-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-15.22.03.32-image.png)

#### 5.2.1.4 类内距离
![2018-11-15.22.04.32-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-15.22.04.32-image.png)

![2018-11-15.22.05.50-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-15.22.05.50-image.png)

#### 5.2.1.5 类内散布矩阵
![2018-11-15.22.11.41-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-15.22.11.41-image.png)

#### 5.2.1.6 两类之间的距离
![2018-11-15.22.15.05-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-15.22.15.05-image.png)

#### 5.2.1.7 各类模式之间的总的均方距离
![2018-11-15.22.16.15-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-15.22.16.15-image.png)

#### 5.2.1.8 多类情况下总的类内、类间及总体散布矩阵
![2018-11-15.22.19.23-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-15.22.19.23-image.png)

#### 5.2.1.9 构造不同的可分性判据
可以用$$S_{W}$$，$$S_{B}$$，$$S_{T}$$构造不同的可分性判据：
![2018-11-15.22.28.12-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-15.22.28.12-image.png)

### 5.2.2 基于概率分布的可分性测度
考完试再加修订

## 5.3 离散K-L变换
离散K-L变换全称Karhunen-Loeve变换（卡洛南-洛伊变换），简单删掉某n-k个特征的做法并不十分理想，因为一般来说，原来的n个数据各自在不同程度上反映了识别对象的某些特征，简单地删去某些特征可能会丢失较多的有用信息。

如果将原来的特征做正交变换，获得的每个数据都是原来n个数据的线性组合，然后从新的数据中选出少数几个，使其尽可能多地反映各类模式之间的差异，而这些特征间又尽可能相互独立，则比单纯的选择方法更灵活、更有效。 

 K-L变换就是一种适用于任意概率密度函数的正交变换。
 
 ### 5.3.1 正交变换
 ![2018-11-15.22.33.02-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-15.22.33.02-image.png)
 ### 5.3.2 K-L展开式
 考完试再加修订
 ### 5.3.3 利用K-L展开式实现特征选择
![2018-11-15.22.47.35-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-15.22.47.35-image.png)
![2018-11-15.22.47.56-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-15.22.47.56-image.png)
### 举例
![2018-11-15.22.49.41-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-15.22.49.41-image.png)
![2018-11-15.22.50.13-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-15.22.50.13-image.png)
![2018-11-15.22.56.21-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-15.22.56.21-image.png)

## 5.4 特征选择
特征选择，就是从n个度量值集合$$\left \{x_{1}, x_{2},…, x_{n} \right \}$$中，按某一准则选取出供分类用的子集，作为降维（m维，m<n）的分类特征。 

### 5.4.1 独立特征的选择准则
1. 使用场合：原始特征测量值统计独立，且类概率密度函数近似正态分布
2. 可分性准则函数为：<br>
![2018-11-16.01.05.24-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-16.01.05.24-image.png)

### 5.4.2 一般特征的选择准则
- 散布矩阵准则
- 散度准则
- Battacharyyaja距离准则 
### 5.4.3 散布矩阵准则
- 类内散布矩阵$$S_{w}$$，类间散布矩阵$$S_{b}$$，总体散布矩阵$$S_{t}$$
- 散布矩阵准则的形式
![2018-11-16.01.08.44-image.png](https://raw.githubusercontent.com/luluhan123/ImageforMarkdown/master/2018-11-16.01.08.44-image.png)